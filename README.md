# Adaptive and Load-Aware Routing for AI Data Center Fabrics

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Mininet](https://img.shields.io/badge/mininet-2.3.0-green.svg)](http://mininet.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

A comprehensive simulation study comparing static ECMP (Equal-Cost Multi-Path) routing with adaptive and load-aware routing schemes for synchronized AI training traffic in leaf-spine data center fabrics.

## üìã Overview

This project implements and evaluates different routing strategies for AI/ML workloads in data center networks:

- **ECMP Routing**: Hash-based static load balancing across equal-cost paths
- **Adaptive Routing**: Dynamic path selection using:
  - Flowlet-based routing for burst tolerance
  - Congestion-aware path selection
  - Hybrid approaches

The simulation uses **Mininet** to create a realistic leaf-spine topology and generates **all-to-all AI training traffic** patterns (AllReduce, Parameter Server, etc.).

## üéØ Project Goals

1. Build a leaf-spine topology in Mininet
2. Implement baseline ECMP routing
3. Add adaptive routing schemes (flowlet and congestion-aware)
4. Generate realistic AI training traffic patterns
5. Compare throughput and tail latency under congestion
6. Analyze load balance and congestion metrics

## üèóÔ∏è Architecture

```
‚îú‚îÄ‚îÄ topologies/
‚îÇ   ‚îî‚îÄ‚îÄ leaf_spine.py          # Leaf-spine topology implementation
‚îú‚îÄ‚îÄ routing/
‚îÇ   ‚îú‚îÄ‚îÄ ecmp_routing.py        # ECMP routing implementation
‚îÇ   ‚îú‚îÄ‚îÄ adaptive_routing.py    # Adaptive routing schemes
‚îÇ   ‚îú‚îÄ‚îÄ traffic_generator.py   # AI traffic pattern generator
‚îÇ   ‚îî‚îÄ‚îÄ monitor.py             # Network monitoring tools
‚îú‚îÄ‚îÄ run_experiment.py          # Main experiment runner
‚îú‚îÄ‚îÄ analyze_results.py         # Results analysis and visualization
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # This file
```

### Topology

**Leaf-Spine Architecture:**
- Configurable number of spine switches (default: 4)
- Configurable number of leaf switches (default: 4)
- Configurable hosts per leaf (default: 4)
- Full mesh connectivity between leaves and spines
- 1 Gbps links with 1ms delay

```
         [Spine 1]  [Spine 2]  [Spine 3]  [Spine 4]
            |  \      /  \      /  \      /  |
            |   \    /    \    /    \    /   |
            |    \  /      \  /      \  /    |
         [Leaf 1]  [Leaf 2]  [Leaf 3]  [Leaf 4]
            |  |      |  |      |  |      |  |
           h1 h2     h3 h4     h5 h6     h7 h8
```

## üöÄ Quick Start

### Prerequisites

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y mininet openvswitch-switch python3-pip iperf3

# Python dependencies
pip3 install -r requirements.txt
```

### Installation

```bash
git clone <repository-url>
cd NAI
chmod +x run_experiment.py analyze_results.py
```

### Running Experiments

#### 1. Run Complete Comparison (ECMP vs Adaptive)

```bash
sudo python3 run_experiment.py --mode comparison --traffic all_to_all --duration 30
```

#### 2. Run Single Experiment

```bash
# ECMP routing
sudo python3 run_experiment.py --mode single --routing ecmp --traffic all_to_all --duration 20

# Adaptive routing
sudo python3 run_experiment.py --mode single --routing adaptive --traffic all_to_all --duration 20
```

#### 3. Custom Topology

```bash
sudo python3 run_experiment.py \
  --mode comparison \
  --spines 6 \
  --leaves 8 \
  --hosts 2 \
  --traffic allreduce \
  --duration 30
```

### Analyzing Results

```bash
# Analyze single experiment
python3 analyze_results.py --mode single --file results/ecmp_all_to_all_*.json

# Compare ECMP vs Adaptive
python3 analyze_results.py \
  --mode compare \
  --ecmp results/ecmp_*.json \
  --adaptive results/adaptive_*.json \
  --plot
```

## üìä Metrics

The simulation collects the following metrics:

### Network Performance
- **Throughput**: Average and per-flow throughput (Mbps)
- **Flow Completion Time (FCT)**: Mean, median, P95, P99 tail latency
- **Link Utilization**: Per-link and aggregate utilization
- **Load Balance**: Standard deviation of link utilization

### Congestion Metrics
- **Packet Drops**: Total and per-link packet drops
- **Queue Length**: Average and maximum queue depths
- **Congestion Duration**: Time spent in congested state

### Traffic Patterns
- **All-to-All**: Every host sends to every other host
- **AllReduce**: Ring-based gradient synchronization
- **Bursty**: Periodic bursts with idle periods

## üìà Expected Results

### ECMP Routing
- ‚úÖ Simple and predictable
- ‚ö†Ô∏è Hash collisions can cause imbalance
- ‚ö†Ô∏è No congestion awareness
- ‚ö†Ô∏è Potential for packet reordering

### Adaptive Routing
- ‚úÖ Better load balance under congestion
- ‚úÖ Reduced tail latency
- ‚úÖ Flowlet-based maintains ordering
- ‚ö†Ô∏è Slightly higher complexity

### Performance Improvements (Expected)
- **Throughput**: 5-15% improvement under congestion
- **Tail Latency (P99)**: 20-40% reduction
- **Load Balance**: 30-50% better distribution
- **Packet Drops**: 40-60% reduction

## üîß Configuration

### Topology Parameters

Edit parameters in `run_experiment.py` or pass via command line:

```python
--spines 4          # Number of spine switches
--leaves 4          # Number of leaf switches
--hosts 4           # Hosts per leaf switch
```

### Traffic Parameters

Configure in `routing/traffic_generator.py`:

```python
duration = 30       # Traffic duration (seconds)
bandwidth = '100M'  # Per-flow bandwidth
protocol = 'tcp'    # 'tcp' or 'udp'
```

### Routing Parameters

Adaptive routing settings in `routing/adaptive_routing.py`:

```python
flowlet_timeout = 0.05      # 50ms flowlet timeout
congestion_threshold = 0.7   # 70% utilization threshold
probe_interval = 0.1        # 100ms monitoring interval
```

## üìù Usage Examples

### Example 1: Quick Test

```bash
# 4x4 topology, 10-second test
sudo python3 run_experiment.py --duration 10
```

### Example 2: Large-Scale Test

```bash
# 8x8 topology, 60-second test with AllReduce traffic
sudo python3 run_experiment.py \
  --spines 8 \
  --leaves 8 \
  --hosts 4 \
  --traffic allreduce \
  --duration 60
```

### Example 3: Bursty Traffic

```bash
# Test with bursty AI inference pattern
sudo python3 run_experiment.py \
  --traffic bursty \
  --duration 30
```

## üìö Documentation

### Key Files

- **[topologies/leaf_spine.py](topologies/leaf_spine.py)**: Mininet topology definition
- **[routing/ecmp_routing.py](routing/ecmp_routing.py)**: ECMP implementation using OpenFlow
- **[routing/adaptive_routing.py](routing/adaptive_routing.py)**: Adaptive routing algorithms
- **[routing/traffic_generator.py](routing/traffic_generator.py)**: AI traffic patterns (AllReduce, etc.)
- **[routing/monitor.py](routing/monitor.py)**: Network statistics collection
- **[run_experiment.py](run_experiment.py)**: Main experiment orchestration
- **[analyze_results.py](analyze_results.py)**: Analysis and visualization

### Traffic Patterns

1. **All-to-All**: Simulates gradient exchange in data-parallel training
2. **AllReduce**: Ring-based collective communication (Horovod-style)
3. **Bursty**: Periodic synchronization with computation gaps

### Routing Algorithms

1. **ECMP**: 5-tuple hash ‚Üí OpenFlow select groups
2. **Flowlet**: Timeout-based path assignment (50ms default)
3. **Congestion-Aware**: Real-time utilization monitoring ‚Üí least-loaded path

## üéì Academic Context

This project addresses key challenges in AI/ML data center networking:

- **Incast Congestion**: Many-to-one traffic patterns
- **Synchronized Traffic**: Periodic barriers in training
- **Tail Latency**: Impact on iteration time
- **Load Imbalance**: Static hashing limitations

**Related Work:**
- HPCC (High Precision Congestion Control)
- pFabric (Priority-based fabric)
- CONGA (Congestion-aware load balancing)
- Flowlet Switching (Burst-level load balancing)

## üìÖ Project Timeline

- **Week 3**: Routing strategy design ‚úÖ
- **Week 6**: Video demonstration of adaptive routing
- **Midterm**: ECMP vs Adaptive demo
- **End-term**: Final report + comprehensive comparison

## üêõ Troubleshooting

### Common Issues

1. **Mininet not found**: `sudo apt-get install mininet`
2. **iperf3 errors**: `sudo apt-get install iperf3`
3. **Permission denied**: Run with `sudo`
4. **OVS connection failed**: `sudo service openvswitch-switch restart`

### Debugging

```bash
# Check Mininet installation
sudo mn --test pingall

# Verify OVS
sudo ovs-vsctl show

# Check network interfaces
sudo mn -c  # Clean up Mininet
```

## üìä Sample Output

```
================================================================================
ECMP vs ADAPTIVE ROUTING COMPARISON
================================================================================

Metric                         ECMP            Adaptive        Change    
------------------------------------------------------------------------------
Throughput (Mbps)              847.32          923.45          +8.99%
Load Balance Score             0.742           0.891           +20.08%
Packet Drops                   1547            423             -72.66%
================================================================================

### Key Findings ###
‚úì Adaptive routing shows significant throughput improvement
‚úì Adaptive routing significantly improves load balance
‚úì Adaptive routing significantly reduces packet drops
```

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- [ ] Implement RocksDB-based flow tracking
- [ ] Add support for priority queuing
- [ ] Integrate with SDN controller (ONOS/OpenDaylight)
- [ ] Support for more traffic patterns
- [ ] Real hardware testbed integration

## üìñ References

1. **ECMP**: RFC 2992 - Analysis of an Equal-Cost Multi-Path Algorithm
2. **Flowlet Switching**: "Better Never than Late: Meeting Deadlines in Datacenter Networks" (SIGCOMM 2013)
3. **CONGA**: "CONGA: Distributed Congestion-Aware Load Balancing for Datacenters" (SIGCOMM 2014)
4. **HPCC**: "HPCC: High Precision Congestion Control" (SIGCOMM 2019)
5. **AllReduce**: "Bringing HPC Techniques to Deep Learning" (Baidu Research)

## üìÑ License

MIT License - See LICENSE file for details

## üë• Authors

Network Architecture & Intelligence Project
AI Data Center Fabric Simulation Study

## üìß Contact

For questions or issues, please open a GitHub issue.

---

**Happy Routing! üöÄ**
